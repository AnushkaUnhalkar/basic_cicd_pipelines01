SECTION A INTERVIEW QUESTIONS
10 Questions 2 Marks Each
Total 20 Marks

1 What is CI CD and why is it important in Machine Learning projects
-CI/CD stands for Continuous Integration and Continuous Deployment/Delivery.
-CI automatically builds, tests, and validates code whenever changes are made.
-CD automatically deploys validated models to production.
-Importance in ML:
 +Automates model training and testing
 +Prevents broken models from deployment
 +Ensures reproducibility
 +Reduces manual errors
 +Enables faster iteration


2 How does CI CD for Machine Learning differ from traditional software CI CD
Traditional CI/CD	
-Tests code only	
-Deterministic output	
-No data dependency	
-Binary artifacts	

ML CI/CD
-Tests code + data + model
-Probabilistic output
-Strong data dependency
-Models, metrics, datasets

ML CI/CD also handles data drift, model validation, and retraining.


3 What are the main stages of a Machine Learning pipeline that should be automated
i.Data ingestion
ii.Data preprocessing
iii.Model training
iv.Model evaluation
v.Model validation
vi.Model versioning
vii.Deployment
viii.Monitoring


4 Explain the role of version control in Machine Learning CI CD
-Tracks code changes (Git)
-Tracks dataset versions
-Tracks model versions
-Enables rollback
-Supports collaboration
-Tools: Git, DVC, MLflow


5 What is data drift and how can CI CD help detect it
Data drift occurs when new production data differs from training data.
CI/CD helps by:
-Running statistical tests
-Comparing distributions
-Failing pipeline if drift exceeds threshold


6 How can model validation be automated in a Continuous Integration pipeline
-Run evaluation scripts
-Compare metrics to thresholds
-Fail pipeline if performance drops
-Example:
assert accuracy >= 0.85


7 What is model versioning and why is it important
Model versioning tracks model changes over time.
Importance:
-Rollback faulty models
-Compare model performance
-Reproducibility
-Audit compliance


8 What are common challenges in implementing CI CD for Machine Learning pipelines
-Data dependency
-Long training times
-Non-deterministic results
-Model performance degradation
-Environment mismatch


9 How can automated testing be applied to Machine Learning models
-Unit tests for preprocessing
-Data validation tests
-Model performance tests
-Inference tests


10 What checks should be performed before deploying a Machine Learning model to production
-Accuracy threshold met:
 +No data leakage
 +Model version tagged
 +Latency tested
 +Bias & fairness checked
 +Logging enabled


SECTION B PRACTICAL QUESTIONS
5 Questions 10 Marks Each
Total 50 Marks

PRACTICAL QUESTION 1
Continuous Integration Design for Model Training

Problem Statement

Design a Continuous Integration workflow for a Machine Learning project where every code change triggers model training and evaluation. The pipeline should stop if model performance falls below a defined threshold.

Expected Folder Structure

ml_ci_project
│
├── data
│ └── dataset.csv
│
├── src
│ ├── train.py
│ └── evaluate.py
│
├── tests
│ └── test_training.py
│
├── reports
│ └── metrics.txt
│
├── requirements.txt
└── README.md

Explain

How the pipeline is triggered
-The pipeline is triggered automatically on every code push or pull request to the repository.
-CI tools like GitHub Actions start the workflow whenever changes are detected.

How performance validation is handled
-After model training, the model is evaluated using predefined metrics such as accuracy.
-A minimum performance threshold is defined, and the pipeline continues only if the model meets this threshold.

How failures are reported
-If model accuracy is below the threshold, the evaluation script raises an error.
-The CI pipeline is marked as failed, and error logs are displayed in the CI tool dashboard.


PRACTICAL QUESTION 2
Model Validation and Approval Pipeline

Problem Statement

Design a CI CD workflow for a classification model where retraining happens on new data and only validated models are approved for deployment.

Expected Folder Structure

ml_cd_validation
│
├── data
│ └── dataset.csv
│
├── src
│ ├── preprocess.py
│ ├── train.py
│ └── validate.py
│
├── model_registry
│ └── model_versions.txt
│
├── logs
│ └── pipeline.log
│
├── requirements.txt
└── README.md

Explain

Validation criteria
-The trained model must achieve an accuracy greater than or equal to the defined threshold (e.g., 80%).
-Models failing to meet the criteria are rejected automatically.

Model approval process
-Approved models are registered in a model registry with version details.
-Only validated and approved models are promoted for deployment.

Difference between CI and CD
-Continuous Integration (CI) focuses on training, testing, and validating the model.
-Continuous Deployment (CD) deploys only the approved models into production.


PRACTICAL QUESTION 3
Automated Retraining Pipeline Design

Problem Statement

Design an automated Machine Learning pipeline that retrains models periodically and stores performance history for comparison.

Expected Folder Structure

ml_automation_project
│
├── data
│ ├── raw
│ └── processed
│
├── src
│ ├── preprocess.py
│ ├── train.py
│ └── evaluate.py
│
├── metrics
│ └── history.csv
│
├── scheduler
│ └── retrain_config.txt
│
└── README.md

Explain

Retraining strategy
-The model is retrained periodically using a scheduler such as cron or GitHub Actions schedule.
-Retraining can also be triggered when new data becomes available.

Performance tracking logic
-Evaluation metrics from each training run are stored in a history file.
-Current model performance is compared with past performance to track improvements or degradation.

Automation benefits
-Reduces manual intervention
-Ensures models stay updated with new data
-Improves consistency and reliability of ML pipelines


PRACTICAL QUESTION 4
Testing Strategy for Machine Learning Pipelines

Problem Statement

Design a testing strategy for a Machine Learning CI pipeline covering data validation model testing and performance testing.

Expected Folder Structure

ml_testing_pipeline
│
├── data
│ └── dataset.csv
│
├── src
│ └── train.py
│
├── tests
│ ├── test_data.py
│ ├── test_model.py
│ └── test_metrics.py
│
├── reports
│ └── test_results.txt
│
└── README.md

Explain

Types of tests used
-Data validation tests to check data quality
-Model tests to verify model creation
-Performance tests to ensure metrics meet thresholds

When tests are executed
-Tests are executed automatically during the CI process after every code change.
-Testing occurs before model training or deployment.

How test failures impact the pipeline
-If any test fails, the pipeline stops immediately.
-The model is not deployed until the issue is fixed.


PRACTICAL QUESTION 5
Monitoring and Performance Comparison Design

Problem Statement

Design a system to monitor Machine Learning model performance across multiple pipeline runs and compare results to detect degradation.

Expected Folder Structure

ml_monitoring_project
│
├── metrics
│ ├── current_metrics.csv
│ └── historical_metrics.csv
│
├── src
│ └── compare_metrics.py
│
├── alerts
│ └── alert_rules.txt
│
└── README.md

Explain

Performance comparison approach
-Current model metrics are compared with historical performance metrics.
-Average or baseline metrics are used for comparison.

How degradation is detected
-If the current model performance is lower than historical averages, degradation is detected.
-Threshold-based rules help identify significant drops.

How alerts are generated
-Alerts are generated when degradation rules are violated.
-Notifications can be logged, printed, or extended to email or monitoring tools.
